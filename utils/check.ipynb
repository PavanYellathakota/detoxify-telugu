{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a8d21f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('../data/processed/toxic_data_cleaned.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e6f61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49172, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5355112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Toxic_flag",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Toxic_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ce6aec24-b39b-4c54-8300-3a2cea8987eb",
       "rows": [
        [
         "0",
         "champu",
         "True",
         "Threatening"
        ],
        [
         "1",
         "champali",
         "True",
         "Threatening"
        ],
        [
         "2",
         "champutha",
         "True",
         "Threatening"
        ],
        [
         "3",
         "champesta",
         "True",
         "Threatening"
        ],
        [
         "4",
         "champestam",
         "True",
         "Threatening"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Toxic_flag</th>\n",
       "      <th>Toxic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>champu</td>\n",
       "      <td>True</td>\n",
       "      <td>Threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>champali</td>\n",
       "      <td>True</td>\n",
       "      <td>Threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>champutha</td>\n",
       "      <td>True</td>\n",
       "      <td>Threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>champesta</td>\n",
       "      <td>True</td>\n",
       "      <td>Threatening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>champestam</td>\n",
       "      <td>True</td>\n",
       "      <td>Threatening</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Text Toxic_flag   Toxic_type\n",
       "0      champu       True  Threatening\n",
       "1    champali       True  Threatening\n",
       "2   champutha       True  Threatening\n",
       "3   champesta       True  Threatening\n",
       "4  champestam       True  Threatening"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2805b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_flag\n",
       "FALSE            44520\n",
       "True              1686\n",
       "False              650\n",
       "TRUE               359\n",
       "Common_Insult        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check no of categories in Toxic flag\n",
    "df['Toxic_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dfca76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Toxic_flag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Toxic_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "6a9b225a-99df-46b3-8edb-ff6f068a9119",
       "rows": [],
       "shape": {
        "columns": 3,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Toxic_flag</th>\n",
       "      <th>Toxic_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Text, Toxic_flag, Toxic_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check toxic flag = 'Common Insult'\n",
    "df[df['Toxic_flag'] == 'Common_Insult']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e35091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the rows if Toxic_flag is 'Common_Insult' to 'TRUE' and update that particular records 'Toxic_type' to 'Common_Insult'\n",
    "df.loc[df['Toxic_flag'] == 'Common_Insult', 'Toxic_flag'] = 'TRUE'\n",
    "df.loc[df['Toxic_flag'] == 'TRUE', 'Toxic_type'] = 'Common_Insult'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9704685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_flag\n",
       "FALSE    44520\n",
       "True      1686\n",
       "False      650\n",
       "TRUE       361\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Toxic_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228f02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also 'Toxic_flag' has 'FALSE' and 'TRUE' values, in various cases, Make all 'Toxic_flag' values to upper case\n",
    "df['Toxic_flag'] = df['Toxic_flag'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42fd3d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_flag\n",
       "FALSE    45170\n",
       "TRUE      2047\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Toxic_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a724079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_type\n",
       "Sexual_Abuse            899\n",
       "none                    650\n",
       "Common_Insult           495\n",
       "Mixed_Toxicity          190\n",
       "Threatening             164\n",
       "Harassment_Bullying     105\n",
       "Profanity_Generic        99\n",
       "Religious_Caste_Slur     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Toxic_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62c9db0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_type\n",
       "Sexual_Abuse            899\n",
       "Common_Insult           495\n",
       "Mixed_Toxicity          190\n",
       "Threatening             164\n",
       "Harassment_Bullying     104\n",
       "Profanity_Generic        99\n",
       "Religious_Caste_Slur     96\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total Toxic comments\n",
    "df[df['Toxic_flag'] == 'TRUE'].shape[0]\n",
    "#total Non Toxic comments\n",
    "df[df['Toxic_flag'] == 'FALSE'].shape[0]\n",
    "#total toxic_type comments\n",
    "df[df['Toxic_flag'] == 'TRUE']['Toxic_type'].value_counts()\n",
    "#print above values in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2069c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the updated dataframe\n",
    "df.to_csv('../data/processed/toxic_data_cleaned.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c602b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af3d4f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c7a02f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\611469672.py:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(process_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset saved with English letters in lowercase.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"../data/raw/toxicity_data.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Function to convert English text to lowercase while keeping other characters as they are\n",
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert only English letters to lowercase\n",
    "        text = re.sub(r'[a-zA-Z]+', lambda x: x.group(0).lower(), text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply the function to all columns in the dataframe\n",
    "df = df.applymap(process_text)\n",
    "\n",
    "# Save the updated dataset back to the same location\n",
    "df.to_csv(dataset_path, index=False)\n",
    "\n",
    "print(\"✅ Dataset saved with English letters in lowercase.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "314f5619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧾 Duplicate groups (first 20 rows):\n",
      "   Text Toxic_flag Toxic_type\n",
      "   abba      false       none\n",
      "   abba      false       none\n",
      "   akka      false       none\n",
      "   akka      false       none\n",
      "   amma      false       none\n",
      "   amma      false       none\n",
      "  atanu      false       none\n",
      "  atanu      false       none\n",
      "  avunu      false       none\n",
      "  avunu      false       none\n",
      "   babu      false       none\n",
      "   babu      false       none\n",
      "   guru      false       none\n",
      "   guru      false       none\n",
      "     he      false       none\n",
      "     he      false       none\n",
      "      i      false       none\n",
      "      i      false       none\n",
      "ivvandi      false       none\n",
      "ivvandi      false       none\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/raw/toxicity_data.csv\")\n",
    "\n",
    "# Identify duplicate texts (including the first occurrence)\n",
    "duplicate_texts = df[df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "# Sort for better readability\n",
    "duplicate_texts_sorted = duplicate_texts.sort_values(by='Text')\n",
    "\n",
    "# Show the head (e.g., first 20 rows)\n",
    "print(\"🧾 Duplicate groups (first 20 rows):\")\n",
    "print(duplicate_texts_sorted.head(20)[['Text', 'Toxic_flag', 'Toxic_type']].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "043e0305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Records before cleaning: 57321\n",
      "✅ Records after cleaning: 57265\n",
      "🗑️ Records removed: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\2527545126.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "path = \"../data/raw/toxicity_data.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Get count before cleaning\n",
    "before_count = len(df)\n",
    "\n",
    "# Find all duplicate texts (including all instances)\n",
    "duplicates_all = df[df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "# Step 1: Group by 'Text'\n",
    "def dedup_logic(group):\n",
    "    if group['Toxic_flag'].any():\n",
    "        # If any are toxic, keep only one toxic\n",
    "        return group[group['Toxic_flag'] == True].head(1)\n",
    "    else:\n",
    "        # Else keep only one non-toxic\n",
    "        return group.head(1)\n",
    "\n",
    "deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n",
    "\n",
    "# Step 2: Get all unique texts from deduped version\n",
    "texts_to_keep = deduped_df['Text'].unique()\n",
    "\n",
    "# Step 3: Remove all records with duplicate Texts, and add back only selected rows\n",
    "df_cleaned = pd.concat([\n",
    "    df[~df['Text'].isin(duplicates_all['Text'])],\n",
    "    deduped_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# Count after cleaning\n",
    "after_count = len(df_cleaned)\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\n✅ Records before cleaning: {before_count}\")\n",
    "print(f\"✅ Records after cleaning: {after_count}\")\n",
    "print(f\"🗑️ Records removed: {before_count - after_count}\")\n",
    "\n",
    "# Optionally save cleaned data back to the same file\n",
    "df_cleaned.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e15426b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\1427623626.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\1427623626.py:29: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Records before cleaning: 57265\n",
      "✅ Records after cleaning: 57265\n",
      "🗑️ Records removed: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load dataset while skipping bad lines\n",
    "path = \"../data/raw/toxicity_data.csv\"\n",
    "df = pd.read_csv(path, on_bad_lines='skip')\n",
    "\n",
    "# Convert all string columns to lowercase\n",
    "df = df.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "\n",
    "# Remove special characters in 'Text' column\n",
    "df['Text'] = df['Text'].str.replace(r\"[.,'\\\"!;()\\[\\]{}<>/%^&_+=\\\\|-]\", \"\", regex=True)\n",
    "\n",
    "# Count before cleaning\n",
    "before_count = len(df)\n",
    "\n",
    "# Find all duplicate texts (including all instances)\n",
    "duplicates_all = df[df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "# Step 1: Group by 'Text'\n",
    "def dedup_logic(group):\n",
    "    if group['Toxic_flag'].any():\n",
    "        # If any are toxic, keep only one toxic\n",
    "        return group[group['Toxic_flag'] == True].head(1)\n",
    "    else:\n",
    "        # Else keep only one non-toxic\n",
    "        return group.head(1)\n",
    "\n",
    "deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n",
    "\n",
    "# Step 2: Remove original duplicate texts and add cleaned unique rows\n",
    "df_cleaned = pd.concat([\n",
    "    df[~df['Text'].isin(duplicates_all['Text'])],\n",
    "    deduped_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# Count after cleaning\n",
    "after_count = len(df_cleaned)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n✅ Records before cleaning: {before_count}\")\n",
    "print(f\"✅ Records after cleaning: {after_count}\")\n",
    "print(f\"🗑️ Records removed: {before_count - after_count}\")\n",
    "\n",
    "# Save the cleaned file to the same location\n",
    "df_cleaned.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9641b8",
   "metadata": {},
   "source": [
    "# combine all csv's \n",
    "toxicity_data --> Text,Toxic_flag,Toxic_type,lang\n",
    "toxicity_data 2 --> Text,User,Red_Flag,Label,Toxic_flag,Toxic_type,lang\n",
    "toxicity_data 3 --> Text,Label,Toxic_flag,Toxic_type,lang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305136af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d5ff694b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/toxicity_data2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m dfs = []\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_bad_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mskip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Normalize missing required columns\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m final_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prudh\\Desktop\\Toxicity_Platform\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prudh\\Desktop\\Toxicity_Platform\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prudh\\Desktop\\Toxicity_Platform\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prudh\\Desktop\\Toxicity_Platform\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\prudh\\Desktop\\Toxicity_Platform\\venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m    876\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    877\u001b[39m             errors=errors,\n\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/raw/toxicity_data2.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "paths = [\n",
    "    \"../data/raw/toxicity_data.csv\",\n",
    "    \"../data/raw/toxicity_data2.csv\",\n",
    "    \"../data/raw/toxicity_data3.csv\"\n",
    "]\n",
    "\n",
    "# Required columns\n",
    "final_columns = ['Text', 'Toxic_flag', 'Toxic_type', 'lang']\n",
    "\n",
    "# Load and harmonize files\n",
    "dfs = []\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path, on_bad_lines='skip')\n",
    "    \n",
    "    # Normalize missing required columns\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df[final_columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine datasets\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Normalize and clean text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text).lower().strip()\n",
    "    text = re.sub(r\"[.,'\\\"!;()\\[\\]{}<>/%^&_+=\\\\|-]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "combined_df['Text'] = combined_df['Text'].apply(clean_text)\n",
    "\n",
    "# Ensure Toxic_flag is boolean\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].astype(str).str.lower().map({'true': True, 'false': False, '1': True, '0': False})\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].fillna(False)\n",
    "\n",
    "# Store original counts\n",
    "original_toxic = combined_df['Toxic_flag'].sum()\n",
    "original_non_toxic = len(combined_df) - original_toxic\n",
    "\n",
    "# Deduplication\n",
    "duplicates_all = combined_df[combined_df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "def dedup_logic(group):\n",
    "    if group['Toxic_flag'].any():\n",
    "        return group[group['Toxic_flag'] == True].head(1)\n",
    "    else:\n",
    "        return group.head(1)\n",
    "\n",
    "deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n",
    "\n",
    "# Merge cleaned duplicates with unique entries\n",
    "final_df = pd.concat([\n",
    "    combined_df[~combined_df['Text'].isin(duplicates_all['Text'])],\n",
    "    deduped_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# New counts\n",
    "new_toxic = final_df['Toxic_flag'].sum()\n",
    "new_non_toxic = len(final_df) - new_toxic\n",
    "\n",
    "# Calculate losses\n",
    "loss_toxic = original_toxic - new_toxic\n",
    "loss_non_toxic = original_non_toxic - new_non_toxic\n",
    "\n",
    "pct_loss_toxic = (loss_toxic / original_toxic * 100) if original_toxic else 0\n",
    "pct_loss_non_toxic = (loss_non_toxic / original_non_toxic * 100) if original_non_toxic else 0\n",
    "\n",
    "# Save to file\n",
    "final_df.to_csv(\"../data/raw/toxicity_data.csv\", index=False)\n",
    "\n",
    "# Report\n",
    "print(\"📊 Summary of Toxic/Non-Toxic Records\")\n",
    "print(f\"Before cleaning:  Toxic: {original_toxic}, Non-Toxic: {original_non_toxic}\")\n",
    "print(f\"After cleaning:   Toxic: {new_toxic}, Non-Toxic: {new_non_toxic}\")\n",
    "print(f\"Records removed:  Toxic: {loss_toxic} ({pct_loss_toxic:.2f}%), Non-Toxic: {loss_non_toxic} ({pct_loss_non_toxic:.2f}%)\")\n",
    "print(f\"✅ Final cleaned dataset saved as 'toxicity_data.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c35cd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Toxic_flag\n",
       "false         45419\n",
       "true          11845\n",
       "toxic_flag        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/raw/toxicity_data.csv', encoding='utf-8')\n",
    "df.head()\n",
    "df.shape\n",
    "df['Toxic_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e649a21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text,Toxic_flag,Toxic_type,lang\n",
      "telugu industry naa modda kudavandi raa,true,sexual_abuse,tenglish\n",
      "ఉండి,false,none,telugu\n",
      "supporting a rauist politician shame,true,mixed_toxicity,english\n",
      "నీ ఎద్దు లాంటి మొఖం చూస్తే నవ్వొస్తుంది idiot,true,political_toxicity,telugu\n",
      "మీ కులం వాళ్ళంతా చచ్చిన కుక్కలు,true,caste_slur,telugu\n",
      "అది చూడగానే నీ బుద్ధి పోiుంది,true,gender_targeted,telugu\n",
      "maciga batch ni samajam lo pedite champeyali,true,religious_caste_slur,tenglish\n",
      "నీ యమ్మని దెం** గుంటా,true,profanity_sexual,telugu\n",
      "text,toxic_flag,toxic_type,lang\n",
      "meeru andharu kalisi desanni nasanam chesthunnaru,true,political_criticism_abusive,telugu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/raw/toxicity_data.csv\")\n",
    "\n",
    "# Filter: Text length < 100\n",
    "df = df[df['Text'].str.len() < 100]\n",
    "\n",
    "# Drop missing required values for fair sampling\n",
    "df = df.dropna(subset=['Text', 'Toxic_flag', 'Toxic_type', 'lang'])\n",
    "\n",
    "# Ensure string types\n",
    "df['Toxic_type'] = df['Toxic_type'].astype(str)\n",
    "df['lang'] = df['lang'].astype(str)\n",
    "\n",
    "# Required categories\n",
    "toxic_flags = df['Toxic_flag'].dropna().unique().tolist()\n",
    "toxic_types = df['Toxic_type'].dropna().unique().tolist()\n",
    "langs = df['lang'].dropna().unique().tolist()\n",
    "\n",
    "# Initialize final set\n",
    "final_records = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# At least one of each toxic_flag\n",
    "for tf in toxic_flags:\n",
    "    sample = df[df['Toxic_flag'] == tf].sample(1, random_state=42)\n",
    "    final_records = pd.concat([final_records, sample])\n",
    "\n",
    "# At least one of each toxic_type\n",
    "for tt in toxic_types:\n",
    "    if tt not in final_records['Toxic_type'].values:\n",
    "        sample = df[df['Toxic_type'] == tt].sample(1, random_state=42)\n",
    "        final_records = pd.concat([final_records, sample])\n",
    "\n",
    "# At least one of each lang\n",
    "for lg in langs:\n",
    "    if lg not in final_records['lang'].values:\n",
    "        sample = df[df['lang'] == lg].sample(1, random_state=42)\n",
    "        final_records = pd.concat([final_records, sample])\n",
    "\n",
    "# Fill remaining to 10\n",
    "remaining = 10 - len(final_records)\n",
    "if remaining > 0:\n",
    "    remaining_samples = df[~df.index.isin(final_records.index)].sample(remaining, random_state=42)\n",
    "    final_records = pd.concat([final_records, remaining_samples])\n",
    "\n",
    "# Drop duplicates, shuffle\n",
    "final_records = final_records.drop_duplicates().sample(10, random_state=42)\n",
    "\n",
    "# Display as CSV-style output\n",
    "print(\"Text,Toxic_flag,Toxic_type,lang\")\n",
    "for _, row in final_records.iterrows():\n",
    "    text = row['Text'].replace(\"\\n\", \" \").replace(\",\", \" \")\n",
    "    print(f\"{text},{row['Toxic_flag']},{row['Toxic_type']},{row['lang']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc764e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ../data/ref/Synthetic\\final_hate_speech_dataset_10000.csv with 10000 records\n",
      "Loaded ../data/ref/Synthetic\\hate_speech_data.csv with 7500 records\n",
      "Loaded ../data/ref/Synthetic\\hate_speech_dataset_10000.csv with 9999 records\n",
      "Loaded ../data/ref/Synthetic\\hate_speech_telugu_fanwars.csv with 1004 records\n",
      "Loaded ../data/ref/Synthetic\\hate_speech_telugu_fanwars_100.csv with 99 records\n",
      "Loaded ../data/ref/Synthetic\\hate_speech_telugu_fanwars_50.csv with 50 records\n",
      "Loaded ../data/ref/Synthetic\\rich_hate_speech_dataset_10000.csv with 11199 records\n",
      "\n",
      "📊 Summary of Toxic/Non-Toxic Records\n",
      "Before cleaning:  Toxic: 32515, Non-Toxic: 7336\n",
      "After cleaning:   Toxic: 9635, Non-Toxic: 1002\n",
      "Records removed:  Toxic: 22880 (70.37%), Non-Toxic: 6334 (86.34%)\n",
      "✅ Final cleaned dataset saved as '../data/ref/Synthetic/toxicity_data.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\1835114511.py:62: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Directory path\n",
    "data_dir = \"../data/ref/Synthetic/\"\n",
    "csv_files = glob(os.path.join(data_dir, \"*.csv\"))\n",
    "\n",
    "# Required columns\n",
    "final_columns = ['Text', 'Toxic_flag', 'Toxic_type', 'lang']\n",
    "\n",
    "# Load and harmonize all CSV files\n",
    "dfs = []\n",
    "for path in csv_files:\n",
    "    df = pd.read_csv(path, on_bad_lines='skip')\n",
    "    print(f\"Loaded {path} with {len(df)} records\")\n",
    "\n",
    "    # Add missing required columns\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "\n",
    "    # Subset to required columns\n",
    "    df = df[final_columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine all CSVs\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Normalize and clean the 'Text' column\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text).lower().strip()\n",
    "    text = re.sub(r\"[.,'\\\"!;:()\\[\\]{}<>/%^&_+=\\\\|@#*$~`?]\", \"\", text)  # remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "combined_df['Text'] = combined_df['Text'].apply(clean_text)\n",
    "\n",
    "# Normalize Toxic_flag values\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].astype(str).str.lower().map({\n",
    "    'true': True, 'false': False, '1': True, '0': False\n",
    "})\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].fillna(False)\n",
    "\n",
    "# Store counts before cleaning\n",
    "original_toxic = combined_df['Toxic_flag'].sum()\n",
    "original_non_toxic = len(combined_df) - original_toxic\n",
    "\n",
    "# Deduplication logic\n",
    "duplicates_all = combined_df[combined_df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "def dedup_logic(group):\n",
    "    if group['Toxic_flag'].any():\n",
    "        return group[group['Toxic_flag'] == True].head(1)\n",
    "    else:\n",
    "        return group.head(1)\n",
    "\n",
    "deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n",
    "\n",
    "# Merge with non-duplicate entries\n",
    "final_df = pd.concat([\n",
    "    combined_df[~combined_df['Text'].isin(duplicates_all['Text'])],\n",
    "    deduped_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# Final counts\n",
    "new_toxic = final_df['Toxic_flag'].sum()\n",
    "new_non_toxic = len(final_df) - new_toxic\n",
    "\n",
    "# Loss calculations\n",
    "loss_toxic = original_toxic - new_toxic\n",
    "loss_non_toxic = original_non_toxic - new_non_toxic\n",
    "\n",
    "pct_loss_toxic = (loss_toxic / original_toxic * 100) if original_toxic else 0\n",
    "pct_loss_non_toxic = (loss_non_toxic / original_non_toxic * 100) if original_non_toxic else 0\n",
    "\n",
    "# Save cleaned file\n",
    "output_path = os.path.join(data_dir, \"toxicity_data.csv\")\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "# Report\n",
    "print(\"\\n📊 Summary of Toxic/Non-Toxic Records\")\n",
    "print(f\"Before cleaning:  Toxic: {original_toxic}, Non-Toxic: {original_non_toxic}\")\n",
    "print(f\"After cleaning:   Toxic: {new_toxic}, Non-Toxic: {new_non_toxic}\")\n",
    "print(f\"Records removed:  Toxic: {loss_toxic} ({pct_loss_toxic:.2f}%), Non-Toxic: {loss_non_toxic} ({pct_loss_non_toxic:.2f}%)\")\n",
    "print(f\"✅ Final cleaned dataset saved as '{output_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ce3494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prudh\\AppData\\Local\\Temp\\ipykernel_26928\\3084917171.py:61: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Summary of Toxic/Non-Toxic Records\n",
      "Before cleaning:  Toxic: 11845, Non-Toxic: 44641\n",
      "After cleaning:   Toxic: 11724, Non-Toxic: 44638\n",
      "Records removed:  Toxic: 121 (1.02%), Non-Toxic: 3 (0.01%)\n",
      "🧹 Removed 121 records containing any of: ['పిచ్చిగా ఉంది', 'నాటకీయంగా ఉన్నాయి ', 'నీ మాటలు నాటకీయంగా']\n",
      "✅ Final cleaned dataset saved as '../data/ref/Synthetic/toxicity_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_dir = \"../data/raw/\"  # Change this path if needed\n",
    "output_file = \"../data/ref/Synthetic/toxicity_data.csv\"\n",
    "final_columns = ['Text', 'Toxic_flag', 'Toxic_type', 'lang']\n",
    "\n",
    "# List of unwanted phrases (add more as needed)\n",
    "unwanted_phrases = ['పిచ్చిగా ఉంది', 'నాటకీయంగా ఉన్నాయి ', 'నీ మాటలు నాటకీయంగా']\n",
    "\n",
    "# === STEP 1: Read and combine all CSVs ===\n",
    "csv_files = glob(os.path.join(input_dir, \"*.csv\"))\n",
    "dfs = []\n",
    "\n",
    "for path in csv_files:\n",
    "    df = pd.read_csv(path, on_bad_lines='skip')\n",
    "\n",
    "    # Add missing required columns if any\n",
    "    for col in final_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = None\n",
    "    df = df[final_columns]\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# === STEP 2: Normalize and clean Text ===\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text).lower().strip()\n",
    "    text = re.sub(r\"[.,'\\\"!;()\\[\\]{}<>/%^&_+=\\\\|-]\", \"\", text)  # remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize whitespace\n",
    "    return text\n",
    "\n",
    "combined_df['Text'] = combined_df['Text'].apply(clean_text)\n",
    "\n",
    "# === STEP 3: Convert Toxic_flag to boolean ===\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].astype(str).str.lower().map({\n",
    "    'true': True, 'false': False, '1': True, '0': False\n",
    "})\n",
    "combined_df['Toxic_flag'] = combined_df['Toxic_flag'].fillna(False)\n",
    "\n",
    "# === STEP 4: Store original counts ===\n",
    "original_toxic = combined_df['Toxic_flag'].sum()\n",
    "original_non_toxic = len(combined_df) - original_toxic\n",
    "\n",
    "# === STEP 5: Remove duplicates with priority for toxic ===\n",
    "duplicates_all = combined_df[combined_df.duplicated(subset='Text', keep=False)]\n",
    "\n",
    "def dedup_logic(group):\n",
    "    if group['Toxic_flag'].any():\n",
    "        return group[group['Toxic_flag'] == True].head(1)\n",
    "    else:\n",
    "        return group.head(1)\n",
    "\n",
    "deduped_df = duplicates_all.groupby('Text', group_keys=False).apply(dedup_logic)\n",
    "\n",
    "# Merge cleaned duplicates with unique entries\n",
    "final_df = pd.concat([\n",
    "    combined_df[~combined_df['Text'].isin(duplicates_all['Text'])],\n",
    "    deduped_df\n",
    "], ignore_index=True)\n",
    "\n",
    "# === STEP 6: Remove rows containing unwanted phrases ===\n",
    "# Clean and normalize the unwanted phrases just like the Text column\n",
    "cleaned_unwanted = [clean_text(p) for p in unwanted_phrases]\n",
    "\n",
    "# Build regex pattern from cleaned phrases\n",
    "regex_pattern = '|'.join(map(re.escape, cleaned_unwanted))\n",
    "\n",
    "# Match cleaned pattern on cleaned Text\n",
    "to_remove = final_df['Text'].str.contains(regex_pattern, na=False)\n",
    "removed_count = to_remove.sum()\n",
    "\n",
    "final_df = final_df[~to_remove]\n",
    "\n",
    "\n",
    "\n",
    "# === STEP 7: New counts and loss percentages ===\n",
    "new_toxic = final_df['Toxic_flag'].sum()\n",
    "new_non_toxic = len(final_df) - new_toxic\n",
    "\n",
    "loss_toxic = original_toxic - new_toxic\n",
    "loss_non_toxic = original_non_toxic - new_non_toxic\n",
    "\n",
    "pct_loss_toxic = (loss_toxic / original_toxic * 100) if original_toxic else 0\n",
    "pct_loss_non_toxic = (loss_non_toxic / original_non_toxic * 100) if original_non_toxic else 0\n",
    "\n",
    "# === STEP 8: Save the cleaned dataset ===\n",
    "final_df.to_csv(output_file, index=False)\n",
    "\n",
    "# === STEP 9: Summary ===\n",
    "print(\"📊 Summary of Toxic/Non-Toxic Records\")\n",
    "print(f\"Before cleaning:  Toxic: {original_toxic}, Non-Toxic: {original_non_toxic}\")\n",
    "print(f\"After cleaning:   Toxic: {new_toxic}, Non-Toxic: {new_non_toxic}\")\n",
    "print(f\"Records removed:  Toxic: {loss_toxic} ({pct_loss_toxic:.2f}%), Non-Toxic: {loss_non_toxic} ({pct_loss_non_toxic:.2f}%)\")\n",
    "print(f\"🧹 Removed {removed_count} records containing any of: {unwanted_phrases}\")\n",
    "print(f\"✅ Final cleaned dataset saved as '{output_file}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cf60333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Removed 0 records containing the phrase 'నాటకీయంగా ఉన్నాయి '\n"
     ]
    }
   ],
   "source": [
    "removed_count = final_df['Text'].str.contains('నాటకీయంగా ఉన్నాయి ', na=False).sum()\n",
    "print(f\"🧹 Removed {removed_count} records containing the phrase 'నాటకీయంగా ఉన్నాయి '\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f208fda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch class paths: []\n",
      "✅ Torch classes path successfully patched.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# Patch to avoid Streamlit poking into torch.classes\n",
    "sys.modules['torch.classes'].__path__ = []\n",
    "# print (\"Torch classes path patched to avoid Streamlit interference.\")\n",
    "# print (\"Torch version:\", torch.__version__)\n",
    "# print the paths to verify\n",
    "print(\"Torch class paths:\", sys.modules['torch.classes'].__path__)\n",
    "# Ensure the patch works by checking if torch.classes is empty\n",
    "if not sys.modules['torch.classes'].__path__:\n",
    "    print(\"✅ Torch classes path successfully patched.\")\n",
    "else:\n",
    "    print(\"❌ Torch classes path patch failed. Current paths:\", sys.modules['torch.classes'].__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fab788fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Comparing List1[0]: 'సూది' vs List2[0]: 'సూది'\n",
      "Unicode Equality: True\n",
      "After Normalization Equality: True\n",
      "UTF-8 Bytes Equality: True\n",
      "\n",
      "Original S1: 'సూది'\n",
      "Codepoints: ['3128 (0xc38)', '3138 (0xc42)', '3110 (0xc26)', '3135 (0xc3f)']\n",
      "UTF-8 Bytes: b'\\xe0\\xb0\\xb8\\xe0\\xb1\\x82\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf'\n",
      "Unicode Form (NFC): సూది\n",
      "Unicode Form (NFD): సూది\n",
      "\n",
      "Original S2: 'సూది'\n",
      "Codepoints: ['3128 (0xc38)', '3138 (0xc42)', '3110 (0xc26)', '3135 (0xc3f)']\n",
      "UTF-8 Bytes: b'\\xe0\\xb0\\xb8\\xe0\\xb1\\x82\\xe0\\xb0\\xa6\\xe0\\xb0\\xbf'\n",
      "Unicode Form (NFC): సూది\n",
      "Unicode Form (NFD): సూది\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def print_unicode_info(label, string):\n",
    "    print(f\"\\n{label}: '{string}'\")\n",
    "    print(\"Codepoints:\", [f\"{ord(c)} ({hex(ord(c))})\" for c in string])\n",
    "    print(\"UTF-8 Bytes:\", string.encode('utf-8'))\n",
    "    print(\"Unicode Form (NFC):\", unicodedata.normalize('NFC', string))\n",
    "    print(\"Unicode Form (NFD):\", unicodedata.normalize('NFD', string))\n",
    "\n",
    "def compare_telugu_lists(list1, list2):\n",
    "    for i, s1 in enumerate(list1):\n",
    "        for j, s2 in enumerate(list2):\n",
    "            print(f\"\\n🔍 Comparing List1[{i}]: '{s1}' vs List2[{j}]: '{s2}'\")\n",
    "\n",
    "            # Normalize both strings\n",
    "            norm_s1 = unicodedata.normalize('NFC', s1)\n",
    "            norm_s2 = unicodedata.normalize('NFC', s2)\n",
    "\n",
    "            # Comparisons\n",
    "            print(f\"Unicode Equality: {s1 == s2}\")\n",
    "            print(f\"After Normalization Equality: {norm_s1 == norm_s2}\")\n",
    "            print(f\"UTF-8 Bytes Equality: {s1.encode('utf-8') == s2.encode('utf-8')}\")\n",
    "\n",
    "            # Print deeper info\n",
    "            print_unicode_info(\"Original S1\", s1)\n",
    "            print_unicode_info(\"Original S2\", s2)\n",
    "\n",
    "# Example inputs from two transliteration methods\n",
    "list1 = [\"సూది\"]  # From suudhi\n",
    "list2 = [\"సూది\"]  # From soodi or another transliterator\n",
    "\n",
    "compare_telugu_lists(list1, list2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c254b68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 'సూది' and 'సూది':\n",
      "Unicode Equality: True\n",
      "After Normalization Equality: True\n",
      "UTF-8 Bytes Equality: True\n",
      "NFC Normalized S1: సూది\n",
      "NFC Normalized S2: సూది\n"
     ]
    }
   ],
   "source": [
    "#Check if these two strings are equivalent in Unicode normalization and UTF-8 encoding\n",
    "import unicodedata\n",
    "def check_equivalence(s1, s2):\n",
    "    norm_s1 = unicodedata.normalize('NFC', s1)\n",
    "    norm_s2 = unicodedata.normalize('NFC', s2)\n",
    "    \n",
    "    print(f\"Comparing '{s1}' and '{s2}':\")\n",
    "    print(\"Unicode Equality:\", s1 == s2)\n",
    "    print(\"After Normalization Equality:\", norm_s1 == norm_s2)\n",
    "    print(\"UTF-8 Bytes Equality:\", s1.encode('utf-8') == s2.encode('utf-8'))\n",
    "    print(\"NFC Normalized S1:\", norm_s1)\n",
    "    print(\"NFC Normalized S2:\", norm_s2)\n",
    "# Example strings to compare\n",
    "s1 = \"సూది\"  # From suudhi\n",
    "s2 = \"సూది\"  # From soodi or another transliterator\n",
    "check_equivalence(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee03ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic_type\n",
      "none                    1999\n",
      "profanity_generic       1000\n",
      "common_insult           1000\n",
      "mixed_toxicity          1000\n",
      "religious_caste_slur    1000\n",
      "sexual_abuse            1000\n",
      "threatening              801\n",
      "harassment_bullying      760\n",
      "gender_targeted          670\n",
      "films_fan_war            633\n",
      "political_toxicity       611\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/prudh/Desktop/Toxicity_Platform/data/training/multi/dataset_multiclass.csv\")\n",
    "print(df['Toxic_type'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
